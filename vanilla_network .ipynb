{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcR0lEQVR4nO3df5BV533f8fcHdpFZWLSOSBOQkBYVpDHRpJaFZEWZJOugZpCKBW1dWYotBVexmqiyG1t1R/llU7nNNHZTt27kysRRZfwL4WRMiIwj1Q5ETjCu0MjSGCnAGq9hC7aMpZV2WTBs9O0f51y4LPfunt29P8/5vGaYuffc55773WeWz332Oec8RxGBmZnl36xmF2BmZo3hwDczKwgHvplZQTjwzcwKwoFvZlYQDnwzs4Jw4Fsmkh6S9Pt1/oydkn49ffwOSU/U4TN+R9Knar3fDJ/7zyUdljQi6epGf74ZOPANkPS4pAcqbF8r6fuSOiLiNyLiw42qKSI+FxG/MpN9SOqTNDhuv38QEb8+s+qm5b8C90bE/Ih4ZvyLSnxA0gFJJyQdkvRfJF2Q9QMkhaRltSq41vuz5nPgG8AjwB2SNG77HcDnImKs8SXlzmXA3gle/zhwN3An0A3cBPwysKX+pVlROPANYCvwE8AvlDZIej2wBtiUPn9E0n9KHy+U9JikIUkvSfq6pFnpa+eMCse97/Xp+34o6eX08SWVCpK0XtLfpo//QzoVUvp3WtIj6WvvkvSCpGFJByX9m3T7POArwOKy9y2WtEHSZ8s+5xZJe9OfZaekN5S9NiDp30t6TtIrkh6V9Loq9c6S9HuSvifpRUmbJF0o6QJJI8Bs4FlJ36nw3uXAPcA7IuIbETEWEXuBfwmslvTLabszU14V+ujJdPOz6c/69tJfOOk01rH053lH2funtL9KP7e1Fwe+EREnSEaSd5ZtvhX4+4h4tsJb7gMGgZ8Efgr4HSDLGh2zgP9NMtq9FDgB/HGG+j6SToXMB94A/JCzI98XSb6YFgDvAj4m6U0RcZxklHyk9N6IOFK+X0lXAF8Afiv9WbYDfylpTlmzW4HVwFLgZ4H1Vcpcn/57C3A5MB/444j4cVo3wD+JiH9c4b2rgMGI+L/jfu7DwG7gn1brm7K2v1j2GfMj4tH0+U8DC4GLgV8DNkq6cgb7szbmwLeSTwP/StLc9Pmd6bZKTgOLgMsi4nREfD0yLMoUET+KiD+PiNGIGAb+M/BLWQtMa9sK/I+I2J7u88sR8Z1I/A3wBGV/qUzi7cCXI+L/RMRpknn2ucANZW0+HhFHIuIl4C+BN1bZ1zuA/xYRByNiBPht4DZJHRnqWAgcrfLa0fT1mfj99Ivnb4Avk3yJWQE58A2AiPhbkpHzWkmXA9cCn6/S/KNAP/BEOo1yf5bPkNQl6ZPptMerwJNAj6TZGcv8U2BfRPxh2T5vkrQ7nVoaAm4me0AuBr5XehIRrwGHSUbDJd8vezxKMnKfdF/p4w6Sv4Amc4zkC7SSRenr0/Vy+tdOeV2LZ7A/a2MOfCu3iWRkfwfwRET8oFKjiBiOiPsi4nLgrcD7Ja1KXx4Fusqa/3TZ4/uAK4E3R8QCoDRtMP5g8XnSL5UrgbvKtl0A/DnJyPynIqKHZFqmtL/J/uo4QjK9VNqfgCXA/5usnsn2RTJlNQZU7MNx/hpYIum68o2SlgDXA19LNx2net9W8/r0eEZ5XaWprensz9qYA9/KbQJuBN5N9ekcJK2RtCwNyFeBf0j/AXwL+FVJsyWt5twpm26SefshST8BfChLUZJuAt4LrEuPN5TMAS4g+ctkLG1XfirnD4CLJF1YZddbgH8maZWkTpIvpB8Du7LUNc4XgPdJWippPvAHwKNZznCKiP3AQ8DnJF2f9t3PkHyZfTUivpo2/RbwL9K/lJZR9uWX+gHJ8YPx/qOkOZJ+geR4xxdnuD9rUw58OyMiBkjCbh6wbYKmy4GvAiPAN4BPRMTO9LV/RzLqHyKZ195a9r7/TjJHfozkYORfZSzt7SQHVV8oO+PmofQ4wHtJgvtl4FfL646IvycJ4oPpWTjnTGVExD7gncD/TGt6K/DWiDiVsa5yDwOfIZmm+i5wEnjPFN5/L/Ap4LMk/fpXwE6SM3VKPgacIgniTwOfG7ePDcCn05+1NE//fZK+OZK2/420X6a7P2tj8g1QzPJJUh/w2YioeOqrFY9H+GZmBTFp4Et6OL2Q5NtVXpekj0vqTy9QeVPtyzQzs5nKMsJ/hOTCk2puIpnTXU5yafj/mnlZZjZTEbHT0zlWbtLAj4gngZcmaLIW2JRe+LKb5LzqaucUm5lZk2S5CnAyF5NcrFIymG4778pBSXeT/BXABRdccM2iRf5eMDObioGBgWMR8ZPTeW8tAr/SRTMVT/2JiI3ARoClS5fGhg0bavDxZmbFsX79+u9N3qqyWpylM0hydWLJJZy9ks/MzFpELQJ/G3BnerbO9cArEVFtISgzM2uSSad0JH0B6AMWKrl70IeAToCIeIhk7ZKbSRbTGiVZotbMzFrMpIEfEbdP8noA/7ZmFZmZWV34Slszs4Jw4JuZFUQtTss0M7M6GhgYqMl+PMI3M2tBvb29DAwM1CzswSN8M7OWUgr4WgZ9iQPfzKyJ6hHs1XhKx8ysCUpTNo3kEb6ZWYPVem4+Kwe+mVkd9fb2ArBz586m1gEOfDOzmhoYGKCvrw9IQr4ZI/lqHPhmZjNUCvVWGs1X4sA3azHHx8TWo90cHu1gSdcY6xYNM6+j4i0mrAWUj+BbaTRfiQPfrIVEwObBBRw60QnA/pE5bB5cwL++7BVU6VZD1nCtHuoTceCbtZADI51nwr7k0IlODox0ckX36SZVZe0c8uV8Hr5Zi4iAHce6Kr6241gX4VmdhqrH0gbN5hG+WYs4MNLJkZOdFV87ctKj/Eap59IGzebAN2sBE43uS3Yc62L5fM/l11oeg70aB75ZC5hodF/iUX5tFCngx/McvlkL2PXS3Jq2s3OVzo8vOo/wzVrA0ZPZ/itmbWdn5e3A60z4t8esBSyZO8aB43MytbPqWv1K12Zz4Ju1gHWLh/nSkW4GRjsZi/OPynYo6O06zbrFw02ornWVRu59fX0tt25NK3Lgm7WA+R3BHZe+2uwy2sL4dWvAI/qsfNDWzNpGO61b04o8wjezluVQry0Hvpm1FId8/XhKx8xahsO+vjzCN7Omcsg3jgPfzBrKAd88ntIxs7rL41LD7cgjfDOrKwd963Dgm1nNeGmD1ubAN7NpGxgYoK+vD8BLG7QBB76ZTcn4pQ08mm8fDnwzm1SlkbtH8+0n01k6klZL2iepX9L9FV6/VNIOSc9Iek7SzbUv1cwazQdc82XSwJc0G3gQuAlYAdwuacW4Zr8HbImIq4HbgE/UulAzawyfQplfWaZ0rgP6I+IggKTNwFrg+bI2ASxIH18IHKllkWZWf6WAd9DnV5bAvxg4XPZ8EHjzuDYbgCckvQeYB9xYaUeS7gbuBrjoooumWquZ1ZjDvViyBP75t99JRvTlbgceiYg/kvRzwGckXRURr53zpoiNwEaApUuXjt+HmdWZA77YsgT+ILCk7PklnD9lcxewGiAiviHpdcBC4MVaFGlm0+OAt3JZztJ5ClguaamkOSQHZbeNa3MIWAUg6Q3A64Af1rJQM8umdH68w97Gm3SEHxFjku4FHgdmAw9HxF5JDwB7ImIbcB/wJ5LeRzLdsz4iPGVj1mA+u8YmkunCq4jYDmwft+2DZY+fB36+tqWZ2UR8patNla+0NWsTXrfGZsqBb9bCvG6N1ZID36zFeN0aqxcHvlmLcKhbvfkWh2ZN5HVrrJE8wjdrAq9bY83gwDdrEIe7NZsD36xOHPDWahz4ZjXigLdW54O2ZjNQOj/erB14hG82TT67xtqNA98sA1/panngwDerwOvWWB458M1SXrfG8s6Bb4U3fvTu0bzllQPfCsmhbkXkwLdCcMCbOfAtpxzwZudz4Fvbc7ibZeMrba0teVlhs6nzCN9anu8AZVYbHuFbS3GQm9WPR/jWchz6ZvXhwLe6qbaSpK9gNWsOB75NW7WReF9fn9efMWtBDnw7o1YB7RG8WWvyQVszs4LwCL8APLViZuDAzy2HvJmN5ymdHHLYm1klDvwcKS03YGZWiQPfzKwgPIdvBvTEEDfwNFfSTzfHGWYe+1jGLq5hSD3NLq+h3Bf55cDPEZ//Pj3L4iC38hidjJ3ZtoDjXMuzvJG9bIk19OvyJlbYOO6LfMs0pSNptaR9kvol3V+lza2Snpe0V9Lna1umTcTLBE9fTwydF3DlOhnjVh6jJ4YaXFnjuS/yb9LAlzQbeBC4CVgB3C5pxbg2y4HfBn4+In4G+K061GpV9Pb2Vl23xiZ2A09XDbiSTsa4gacbVFHzuC/yL8sI/zqgPyIORsQpYDOwdlybdwMPRsTLABHxYm3LtCz6+vqaXULbuZL+mrZrZ+6L/MsS+BcDh8ueD6bbyl0BXCHp7yTtlrS60o4k3S1pj6Q9w8PD06vYrIa6OZ6p3XxG61xJ87kv8i9L4KvCthj3vANYDvQBtwOfks4/nB8RGyNiZUSs7O7unmqtZjU3zLxM7UboqnMlzee+yL8sgT8ILCl7fglwpEKbv4iI0xHxXWAfyReAWUvbx7Katmtn7ov8yxL4TwHLJS2VNAe4Ddg2rs1W4C0AkhaSTPEcrGWhZvWwi2s4PcnZyafpYBcrG1RR87gv8m/SwI+IMeBe4HHgBWBLROyV9ICkW9JmjwM/kvQ8sAP4QET8qF5FW2U+NXPqhtTDFtZUDbrTdLCFNQzpwgZX1njui/xTxPjp+MZYunRpbNiwoSmfXQQO/6kpv7p0PqOM0JVeXbqycAHnvmhtGzZseDoipvVnlq+0NSMZ3W5nFdtZ1exSms59kV9ePM3MrCAc+GZmBeHAz6ne3l5feWtm53Dg59jAwIBD38zOcODnnM/WMbMSB34BeDVNMwMHvplZYTjwzcwKwoFfIJ7WMSs2B37BeD7frLgKu7TC8TGx9Wg3h0c7WNI1xrpFw8zraM66Qo3W29vrs3fMCqiQI/wI2Dy4gP0jczjx2iz2j8xh8+ACmrSOXMM57M2KqZCBf2Ckk0MnOs/ZduhEJwdGOqu8I388tWNWPIUL/AjYcazyLdp2HOsqzCjfzIqncIF/YKSTIycrj+SPnCzWKB985o5ZkRQq8Cca3ZcUcZTv9XbMiqFQgT/R6L6kiKN8L7JmVgyFCvxdL82tabs82blzZ7NLMLM6K9R5+EdPZvtxs7bLk/K5fJ+2aZZPhRrhL5k7VtN2ZmbtpFCBv27xMMvmnaJDlY/KdihYNu8U6xYPN7gyM7P6K9TcxfyO4I5LX212GS3PSy+Y5VOhRviWna/ENcsfB75V5VG+Wb448K2q3t5en59vliMOfJvQwMCAp3bMcsKBb5k49M3anwPfMvP0jll7c+BbZj6Ia9beHPg2JZ7aMWtfDnybMp+jb9aeHPg2LQ58s/ZTqKUVrLLjY2Lr0W4Oj3awpGuMdYuGmdcx8V1gajmf3xND3MDTXEk/3RxnmHnsYxm7uIYh9dTsc8yKziP8gouAzYML2D8yhxOvzWL/yBw2Dy7IdNevWozyl8VB7mET1/IsCziOgAUc51qe5R42sSwOzvgzzCyRKfAlrZa0T1K/pPsnaPc2SSFpZe1KtHo6MNLJoRPn3uHr0Insd/2ayXx+TwxxK4/RSeXlqDsZ41YeoyeGprV/MzvXpIEvaTbwIHATsAK4XdKKCu26gfcC36x1kVYfE93jtxH39r2Bp6uGfUknY9zA0/UtxKwgsozwrwP6I+JgRJwCNgNrK7T7MPAR4GQN67M6mugev1O9t+901t25kv6atjOziWUJ/IuBw2XPB9NtZ0i6GlgSEY9NtCNJd0vaI2nP8LBvMtJME43uS6Y6yp/qujvdHM/Ubj6j2Ysws6qyBL4qbDsTA5JmAR8D7ptsRxGxMSJWRsTK7u7u7FVazU00ui+Z6ii/JGvoDzMvU7sRJv5iMrNssgT+ILCk7PklwJGy593AVcBOSQPA9cA2H7htbbtemlvTduWyBv4+ltW0nZlNLEvgPwUsl7RU0hzgNmBb6cWIeCUiFkZEb0T0AruBWyJiT10qtpo4ejLbJRhZ25XLeo7+Lq7h9CSXgpymg1147GBWC5MGfkSMAfcCjwMvAFsiYq+kByTdUu8CrT6WzJ347Jipthsvyyh/SD1sYU3V0D9NB1tYw5AunFYNZnauTMO3iNgObB+37YNV2vbNvCyrt3WLh/nSkW4GRjsZi/MP03Qo6O06zbrF0z+4XjpHf+fOnVXb9OtyPhF3nrnSdj6jjNCVXmm70mFvVkNeWqGg5ncEd1z6at0/p3TmzkTTPEPqYTur2M6qutdjVmReWsEawoutmTWfA98axnfMMmsuB741zERz+WZWfw58axhP65g1lw/aWkOVh77vkWvWWB7hW9N4xG/WWA58axqP8M0ay4FvTeOboZs1lgPfms6hb9YYDnwzs4LwWTrWEnz2TqInhs6sK9TNcYaZl64rdA1D6ml2edbmHPhmLWJZHDzvpu4LOM61PMsb2cuWWEO/Lm9ihdbuPKVj1gJ6Yui8sC/XyRi38hg9MdTgyixPHPjWcop4EPcGnq4a9iWdjHEDTzeoIssjB761pKKF/pX017SdWSUOfLMW0M3xTO3mM1rnSizPHPjWsnp7ewuzpPIw8zK1G6GrzpVYnjnwraWV7piVd/tYVtN2ZpU48K0t5D30d3FN1Zu5l5ymg12sbFBFlkcOfGsbeQ79IfWwhTVVQ/80HWxhjW/qbjPiC6/MWkS/LucTceeZK23nM8oIXemVtisd9jZjDnyzFjKkHraziu2sanYplkMOfGsrXnPHbPo8h29mVhAOfGtbeT6Ia1YPDnxra75rlll2Dnxre57LN8vGgW9tzyN8s2x8lo7lgs/eMZucR/hmZgXhwLfcKcoKm2ZT5SkdawnHx8TWo90cHu1gSdcY6xYNM68jprUvT+lYreTtpvIe4VvTRcDmwQXsH5nDiddmsX9kDpsHFxDTy3vAB3Jt5pbFQe5hE9fyLAs4jjh7U/l72MSyONjsEqcsU+BLWi1pn6R+SfdXeP39kp6X9Jykr0m6rPalWl4dGOnk0InOc7YdOtHJgZHOKu/Ipkg3ULHayutN5ScNfEmzgQeBm4AVwO2SVoxr9gywMiJ+Fvgz4CO1LtTyKQJ2HKt8F6cdx7pmNMqH4txAxWorrzeVzzLCvw7oj4iDEXEK2AysLW8QETsionSzzd3AJbUt0/LqwEgnR05WHskfOTnzUX6JQ9+mIq83lc8S+BcDh8ueD6bbqrkL+EqlFyTdLWmPpD3Dw8PZq7Rcmmh0X1KLUX6Jp3csq7zeVD5L4KvCtor/BSW9E1gJfLTS6xGxMSJWRsTK7u7u7FVaLk00ui+p5Sjf0zuWVV5vKp8l8AeBJWXPLwGOjG8k6Ubgd4FbIuLHtSnP8mzXS3Nr2i4rj/RtMnm9qXyWwH8KWC5pqaQ5wG3AtvIGkq4GPkkS9i/WvkzLo6Mns10GkrVdVgMDAw59m1Bebyo/aeBHxBhwL/A48AKwJSL2SnpA0i1ps48C84EvSvqWpG1Vdmd2xpK5E58FMdV2U+HpHZtIXm8qn2noFBHbge3jtn2w7PGNNa7LCmDd4mG+dKSbgdFOxuL8Q0UdCnq7TrNucX0O8PuKXJtIHm8q76UVrGnmdwR3XPpq0z7fK2zaZPJ2U3kvrWCGD+RaMTjwzYCdO3c2uwSzuvOUjhme3rFi8AjfzKwgHPhmZgXhKR2zcTy9Y3nlEb5ZFb4wy/LGgW9WhZdgsLxx4JtNwEswWJ448M3MCqKwB22Pj4mtR7s5PNrBkq4x1i0aZl5Hje60YbnT29tLb2+vL9CytlbIEX4EbB5cwP6ROZx4bRb7R+aweXBBze6sZPnkM3as3RUy8A+MdHLoxLl3UTp0onZ3VrL88ny+tbPCBf5E91Gt5f1TLb9K0ztm7aZwgT/RfVRref9UM7NWU6jAn2h0X+JRvpnlVaECf6LRfYlH+ZZVaWrHF2dZuyhU4O96aW5N25mB19K39lGowD96MttlB1nbmYHP3LH2UajAXzJ3rKbtzEo8tWPtoFCBv27xMMvmnaJDlY/KdihYNu8U6xYPN7gyywOvu2OtrlBzF/M7gjsufbXZZZiZNUWhRvhmjeBRvrUqB75ZHfhqXGtFDnyzOvKBXGslDnwzs4Jw4JvVkc/csVbiwDdrAJ+nb63AgW/WIF6CwZrNgW/WID5zx5rNgW/WYJ7esWZx4Js1gQ/mWjM48M3MCsKBb2ZWEJkCX9JqSfsk9Uu6v8LrF0h6NH39m5J6a12oWR75QK410qSBL2k28CBwE7ACuF3SinHN7gJejohlwMeAP6x1oWZmNjNZRvjXAf0RcTAiTgGbgbXj2qwFPp0+/jNglSTVrkyzfPNI3xohy3r4FwOHy54PAm+u1iYixiS9AlwEHCtvJOlu4O706Y/Xr1//7ekUnUMLGddXBea+OMt9cZb74qwrp/vGLIFfaaQ+/pZRWdoQERuBjQCS9kTEygyfn3vui7PcF2e5L85yX5wlac9035tlSmcQWFL2/BLgSLU2kjqAC4GXpluUmZnVXpbAfwpYLmmppDnAbcC2cW22Ab+WPn4b8NcRUfnGsWZm1hSTTumkc/L3Ao8Ds4GHI2KvpAeAPRGxDfhT4DOS+klG9rdl+OyNM6g7b9wXZ7kvznJfnOW+OGvafSEPxM3MisFX2pqZFYQD38ysIOoe+F6W4awMffF+Sc9Lek7S1yRd1ow6G2Gyvihr9zZJISm3p+Rl6QtJt6a/G3slfb7RNTZKhv8jl0raIemZ9P/Jzc2os94kPSzpRUkVr1VS4uNpPz0n6U2ZdhwRdftHcpD3O8DlwBzgWWDFuDb3AA+lj28DHq1nTc36l7Ev3gJ0pY9/s8h9kbbrBp4EdgMrm113E38vlgPPAK9Pn/+jZtfdxL7YCPxm+ngFMNDsuuvUF78IvAn4dpXXbwa+QnIN1PXAN7Pst94jfC/LcNakfREROyJiNH26m+SahzzK8nsB8GHgI8DJRhbXYFn64t3AgxHxMkBEvNjgGhslS18EsCB9fCHnXxOUCxHxJBNfy7QW2BSJ3UCPpEWT7bfegV9pWYaLq7WJiDGgtCxD3mTpi3J3kXyD59GkfSHpamBJRDzWyMKaIMvvxRXAFZL+TtJuSasbVl1jZemLDcA7JQ0C24H3NKa0ljPVPAGyLa0wEzVbliEHMv+ckt4JrAR+qa4VNc+EfSFpFsmqq+sbVVATZfm96CCZ1ukj+avv65KuioihOtfWaFn64nbgkYj4I0k/R3L9z1UR8Vr9y2sp08rNeo/wvSzDWVn6Akk3Ar8L3BIRP25QbY02WV90A1cBOyUNkMxRbsvpgdus/0f+IiJOR8R3gX0kXwB5k6Uv7gK2AETEN4DXkSysVjSZ8mS8ege+l2U4a9K+SKcxPkkS9nmdp4VJ+iIiXomIhRHRGxG9JMczbomIaS8a1cKy/B/ZSnJAH0kLSaZ4Dja0ysbI0heHgFUAkt5AEvg/bGiVrWEbcGd6ts71wCsRcXSyN9V1SifqtyxD28nYFx8F5gNfTI9bH4qIW5pWdJ1k7ItCyNgXjwO/Iul54B+AD0TEj5pXdX1k7Iv7gD+R9D6SKYz1eRwgSvoCyRTewvR4xYeAToCIeIjk+MXNQD8wCrwr035z2FdmZlaBr7Q1MysIB76ZWUE48M3MCsKBb2ZWEA58M7OCcOCbmRWEA9/MrCD+PzMXBV3odFt+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sys\n",
    "from timeit import default_timer as dt\n",
    "from math import e\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def activate(x,W,b):\n",
    "    '''Defines our activation\n",
    "       function based on the \n",
    "       sigmoid activation.\n",
    "       Takes in the previous neurons\n",
    "       connected outputs x / our \n",
    "       weights W / our bias vector\n",
    "       b'''\n",
    "    return(1/(1+np.exp(-(np.dot(W,x)+b)))) #entire exponent is negative is key\n",
    "\n",
    "def cost(W2,W3,W4,b2,b3,b4,x1,x2,y):\n",
    "    '''Takes in our weight matrices\n",
    "       and bias vectors and computes\n",
    "       the cost function as defined \n",
    "       in the literature as our final\n",
    "       neurons values, a4 below,\n",
    "       in respect to the true output\n",
    "       squared\n",
    "       x1 - data\n",
    "       x2 - true labels\n",
    "       y - predicted points\n",
    "       Cost is run through Niter #\n",
    "       of times \n",
    "       \n",
    "    '''\n",
    "    costvector = np.zeros((10, 1))\n",
    "    x = np.zeros((2,  1))\n",
    "    for i in np.arange(costvector.shape[0]):\n",
    "        x[0,0], x[1,0] = x1[i], x2[i]\n",
    "        a2 = activate(x,  W2, b2)\n",
    "        a3 = activate(a2, W3, b3)\n",
    "        a4 = activate(a3, W4, b4)\n",
    "        costvector[i] = np.linalg.norm(a4.ravel()-y[:,i], 2)\n",
    "\n",
    "    return np.linalg.norm(costvector, 2)**2\n",
    "\n",
    "def inference(W2, W3, W4, b2, b3, b4, x_vec):\n",
    "    '''Takes in adjusted weights and \n",
    "       biases from cost function that\n",
    "       have run through our network and\n",
    "       return an output layer of \n",
    "       a^[L] = a^4 as the predicted\n",
    "       measure\n",
    "       Takes in processed weights\n",
    "       and biases and 'activates'\n",
    "       through the network using\n",
    "       input xvector data points\n",
    "       and produces a^Lth neuron \n",
    "       for our prediction\n",
    "    '''\n",
    "    a2 = activate(xvec, W2, b2)\n",
    "    a3 = activate(a2,   W3, b3)\n",
    "    a4 = activate(a3,   W4, b4)\n",
    "\n",
    "    return a4\n",
    "\n",
    "\n",
    "x1 = np.array([0.1, 0.3, 0.1, 0.6, 0.4, 0.6, 0.5, 0.9, 0.4, 0.7]) #data\n",
    "x2 = np.array([0.1, 0.4, 0.5, 0.9, 0.2, 0.3, 0.6, 0.2, 0.4, 0.6]) \n",
    "\n",
    "y           = np.zeros((2, 10)) #labels (fires or does not fire) \n",
    "y[0:1, 0:5] = np.ones((1,  5))\n",
    "y[0:1, 5: ] = np.zeros((1, 5))\n",
    "y[0:1, 5: ] = np.zeros((1, 5))\n",
    "y[1: , 5: ] = np.ones((1,  5))\n",
    "\n",
    "#Normal randomization used to initialize weight matrices and biases\n",
    "W2 =  npr.uniform(size = (2,2)) #where 1st arg is the current layer size, 2nd is previous arg size\n",
    "W3 =  npr.uniform(size = (3,2)) \n",
    "W4 =  npr.uniform(size = (2,3))\n",
    "\n",
    "b2 = npr.uniform(size = (2,1))\n",
    "b3 = npr.uniform(size = (3,1))\n",
    "b4 = npr.uniform(size = (2,1))\n",
    "\n",
    "eta = .45                                # our learning rate in paper is .05, tried .4 to improve\n",
    "                                        # our cost \n",
    "Niter = 100000                          #number of SG iterations\n",
    "\n",
    "savecost = np.zeros((Niter,1))           #save the value of our cost function\n",
    "\n",
    "xvec = np.zeros((2,1))\n",
    "yvec = np.zeros((2,1))\n",
    "\n",
    "\n",
    "for counter in range(Niter):\n",
    "    k = npr.randint(10)                     #choose a training node\n",
    "    xvec[0,0], xvec[1,0] = x1[k], x2[k]     #populate our input vectors\n",
    "    yvec[:,0] = y[:, k]                     #define our randomly selected node for loss purposes\n",
    "    \n",
    "    #forward pass algorithm below\n",
    "    a2 = activate(xvec,W2,b2)               #first neuron has input vector of data \n",
    "    a3 = activate(a2,W3,b3)                 #second neuron uses a^1 as input data\n",
    "    a4 = activate(a3,W4,b4)\n",
    "    \n",
    "     #backward pass now\n",
    "    \n",
    "    delta4 = a4*(1 - a4)*(a4-yvec)            #delta, our loss term, calculated from final neuron\n",
    "                                              #along with our randomly chosen node and correct label value\n",
    "    delta3 = a3*(1 - a3)* np.dot(W4.T,delta4) #these are defined such that they abide by\n",
    "    delta2 = a2*(1 - a2)* np.dot(W3.T,delta3) #the loss partial derivate rules prove in section 5\n",
    "     #gradient step\n",
    "    W2 -= eta*delta2*xvec.T             #update our Weights and biases using loss function, or delta\n",
    "    W3 -= eta*delta3*a2.T               #weights and biases calculated and then entered into cost function to verify decay\n",
    "    W4 -= eta*delta4*a3.T\n",
    "    b2 -= eta*delta2\n",
    "    b3 -= eta*delta3\n",
    "    b4 -= eta*delta4\n",
    "    \n",
    "    savecost[counter] = cost(W2,W3,W4,b2,b3,b4,x1,x2,y)  #store our cost to observe\n",
    "    \n",
    "#We are predicting a 'fire' or 'not fire' to predict an x or O on on the map given an\n",
    "#ordered pair\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(savecost)\n",
    "plt.xlabel('Gradient Steps')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function')\n",
    "plt.show\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200))\n",
    "\n",
    "X1 = np.array(X.ravel())\n",
    "X2 = np.array(Y.ravel())\n",
    "\n",
    "XTest  = np.stack((X1, X2), axis = 1)\n",
    "empty  = np.zeros(200*200)\n",
    "\n",
    "\n",
    "for i in np.arange(XTest.shape[0]):\n",
    "\n",
    "    xvec[0,0], xvec[1,0] = XTest[i, 0], XTest[i, 1]\n",
    "\n",
    "\n",
    "    YPredictions = inference(W2, W3, W4, b2, b3, b4, xvec)\n",
    "    YPredictions = np.array(YPredictions[0] >= YPredictions[1])\n",
    "\n",
    "\n",
    "    if YPredictions[0] == True:\n",
    "        empty[i] = 1\n",
    "\n",
    "\n",
    "Pred = empty.reshape((200, 200))\n",
    "plt.figure()\n",
    "plt.title('Visualization of Output')\n",
    "plt.contourf(X,Y,Pred,colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
    "\n",
    "plt.scatter(x1[0:5], x2[0:5], marker='^', lw=5)\n",
    "plt.scatter(x1[5:],  x2[5:], marker='o', lw=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import sys\n",
    "from timeit import default_timer as dt\n",
    "from math import e\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def activate(x,W,b):\n",
    "    '''Defines our activation\n",
    "       function based on the \n",
    "       sigmoid activation.\n",
    "       Takes in the previous neurons\n",
    "       connected outputs x / our \n",
    "       weights W / our bias vector\n",
    "       b'''\n",
    "    return(1/(1+np.exp(-(np.dot(W,x)+b)))) #entire exponent is negative is key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) $\\hspace{5mm}$ Here we define the backbone of our neural network, which is the activation function. Here we utilize the sigmoid function, or $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ which represents a smoothed step function of sorts. Passing through weight matrices, $W$, and bias vectors, $b$, which hold the values to shift and stretch each neuron as it learns through each gradient iteration ultimately enables the firing of proper neurons to accurately make inferences on a desired data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W2,W3,W4,b2,b3,b4,x1,x2,y):\n",
    "    '''Takes in our weight matrices\n",
    "       and bias vectors and computes\n",
    "       the cost function as defined \n",
    "       in the literature as our final\n",
    "       neurons values, a4 below,\n",
    "       in respect to the true output\n",
    "       squared\n",
    "       x1 - data\n",
    "       x2 - true labels\n",
    "       y - predicted points\n",
    "       Cost is run through Niter #\n",
    "       of times \n",
    "       \n",
    "    '''\n",
    "    costvector = np.zeros((10, 1))\n",
    "    x = np.zeros((2,  1))\n",
    "    for i in np.arange(costvector.shape[0]):\n",
    "        x[0,0], x[1,0] = x1[i], x2[i]\n",
    "        a2 = activate(x,  W2, b2)\n",
    "        a3 = activate(a2, W3, b3)\n",
    "        a4 = activate(a3, W4, b4)\n",
    "        costvector[i] = np.linalg.norm(a4.ravel()-y[:,i], 2)\n",
    "\n",
    "    return np.linalg.norm(costvector, 2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) $\\hspace{5mm}$ Here we see the installment of the cost function, which is used later in the derivation of our $\\delta$ functions for use in forward and backward propogation for the means of updating our weight and bias matrices used to shift and stretch our activation function. For now, our cost function is modeled after the equation $$Cost = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{2}||y(x^{{i}}) - a^{[L]}(x^{{i}})||_{2}^{2}$$\n",
    "\n",
    "where $a^{l}$ runs from $l=2,3,...L$ and represents a neron at layer $l$ for $L$ being our final neuron. $a^{l}$ can be seen as the function describing the output, or the firing or not firing, of neuron $l$ as described by $a^{l} = \\sigma(W^{l}a^{l-1}+b^{l}) \\in \\mathbb{R}^{n_{l}}$ based on the output of the previous neuron and the current weights and biases. Once run through the entire gradient step of the network, the loss is calculated from the activation pattern of our last neuron layer to be compared against our actual values int eh stored $y$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(W2, W3, W4, b2, b3, b4, x_vec):\n",
    "    '''Takes in adjusted weights and \n",
    "       biases from cost function that\n",
    "       have run through our network and\n",
    "       return an output layer of \n",
    "       a^[L] = a^4 as the predicted\n",
    "       measure\n",
    "       Takes in processed weights\n",
    "       and biases and 'activates'\n",
    "       through the network using\n",
    "       input xvector data points\n",
    "       and produces a^Lth neuron \n",
    "       for our prediction\n",
    "    '''\n",
    "    a2 = activate(xvec, W2, b2)\n",
    "    a3 = activate(a2,   W3, b3)\n",
    "    a4 = activate(a3,   W4, b4)\n",
    "\n",
    "    return a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) $\\hspace{5mm}$ Here is a simple inference step once our weights and biases are properly trained according to our desired step count and learning rate. This takes in our weights and biases and our x_vec we wish to make predictions on and run them through our activation neurons to determine their grouping on our 2-D plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0.1, 0.3, 0.1, 0.6, 0.4, 0.6, 0.5, 0.9, 0.4, 0.7]) #data\n",
    "x2 = np.array([0.1, 0.4, 0.5, 0.9, 0.2, 0.3, 0.6, 0.2, 0.4, 0.6]) \n",
    "\n",
    "y           = np.zeros((2, 10)) #labels (fires or does not fire) \n",
    "y[0:1, 0:5] = np.ones((1,  5))\n",
    "y[0:1, 5: ] = np.zeros((1, 5))\n",
    "y[0:1, 5: ] = np.zeros((1, 5))\n",
    "y[1: , 5: ] = np.ones((1,  5))\n",
    "\n",
    "#Normal randomization used to initialize weight matrices and biases\n",
    "W2 =  npr.uniform(size = (2,2)) #where 1st arg is the current layer size, 2nd is previous arg size\n",
    "W3 =  npr.uniform(size = (3,2)) \n",
    "W4 =  npr.uniform(size = (2,3))\n",
    "\n",
    "b2 = npr.uniform(size = (2,1))\n",
    "b3 = npr.uniform(size = (3,1))\n",
    "b4 = npr.uniform(size = (2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) $\\hspace{5mm}$ We have above an initialization of our data points on our 2-D plane through the $x1$ and $x2$ arrays. From there, the data labels are constructed in our $y$ arrays as either x's or o's as represented by either a 1 or a 0. The standard practice of initializing our weights and biases is done through sampling from a uniform distribution. This gives some sort of starting value for our network to at least begin conducting gradient losses at the beginning of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = .4                                # our learning rate in paper is .05, tried .4 to improve\n",
    "                                        # our cost \n",
    "Niter = 100000                          #number of SG iterations\n",
    "\n",
    "savecost = np.zeros((Niter,1))           #save the value of our cost function\n",
    "\n",
    "xvec = np.zeros((2,1))\n",
    "yvec = np.zeros((2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) $\\hspace{5mm}$ The most important mathematical topic in this block is the $\\eta$ variable, which represents our learning rate and allows us to go into the deeper mathematics of stochastic gradient descent and its use in our network. $\\eta$ comes into play with updating our cost function given our vector $p$, which is defined as the general vector of our weights, $W$, and our biases, $b$. As background, our cost function as we perturb our $p$ values is defined as $$Cost(p + \\Delta p) \\approx Cost(p) + \\sum_{r=1}^{s}\\frac{\\partial Cost(p)}{\\partial p_{r}}\\Delta p_{r}$$ in respect to our $r^{th}$ parameter. Since we wish to update our weights and biases in the direction of negative of our cost value evaluated at $p$, we then get the update of $$p \\rightarrow p - \\eta \\nabla Cost(p)$$ where $$(\\nabla Cost(p))_{r} = \\frac{\\partial Cost(p)}{\\partial p_{r}}$$ at our $r^{th}$ parameter yet again. Thus, our $\\eta$ variable is key in possibly erasing the issue of falling into a trough with our loss function as seen by a stagnant cost function at a relatively high value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(Niter):\n",
    "    k = npr.randint(10)                     #choose a training node\n",
    "    xvec[0,0], xvec[1,0] = x1[k], x2[k]     #populate our input vectors\n",
    "    yvec[:,0] = y[:, k]                     #define our randomly selected node for loss purposes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) $\\hspace{5mm}$ Here, we begin our loop at our prescribed number of gradient steps, with the most important aspect of this block being our $k$ variable. This represents a cheaper alternative to the mean of the individual gradients over all our training points by instead randomly choosing a training point from our available nodes of our network. This is seen by our $yvec$, which is used later on in our backpropigation, as being populated with our randomly chosen single node for the purpose of calculating our cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass algorithm below\n",
    "a2 = activate(xvec,W2,b2)               #first neuron has input vector of data \n",
    "a3 = activate(a2,W3,b3)                 #second neuron uses a^1 as input data\n",
    "a4 = activate(a3,W4,b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) $\\hspace{5mm}$ Here is the beginning of the training of our network utilizing much of the initialization of our weights, biases, and training data described in prior blocks. With notation lifted from the paper, $[a2,a3,a4]$ represent the neurons of our neural network, with our first neuron taking in our $xvec$ of training data as its input vector. The sigmoid activation, as outlined in our first block, is used here to take in our weights an biases and produce a value along the shifted curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #backward pass now\n",
    "    \n",
    "    delta4 = a4*(1 - a4)*(a4-yvec)            #delta, our loss term, calculated from final neuron\n",
    "                                              #along with our randomly chosen node and correct label value\n",
    "    delta3 = a3*(1 - a3)* np.dot(W4.T,delta4) #these are defined such that they abide by\n",
    "    delta2 = a2*(1 - a2)* np.dot(W3.T,delta3) #the loss partial derivate rules prove in section 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) $\\hspace{5mm}$ This is where the bulk of the mathematics and derivations come into play for our network and where the gradient vector with our cost function and weighted inputs becomes used define delta values above, or $\\delta$ error values. \n",
    "\n",
    "Let $z^{[l]} = W^{[l]}a^{l-1]} + b^{[l]} \\in R^{n_{l}}$ is the input for our $l^{th}$ neuron, and we can represent our subsequent neuron as $a^{[l]} = \\sigma(z^{[l]})$ for $l = 2,3,...L$. Let us also represent our cost here as $C = \\frac{1}{2}||y - a^[L]||_{2}^{2}$ for our fixed, randomly chosen, training point. Our error expression at our $j^{th}$ neuron an at our $l^{th}$ layer is to be defined as $$\\delta_{j}^{[l]} = \\frac{\\partial C}{\\partial z_{j}^{[l]}} \\textrm{ for } 1 \\leq j \\leq n_{l} \\textrm{ and } 2 \\leq l \\leq L$$ \n",
    "\n",
    "Where $L$ again is the last layer of our network. This error term is key in measuring the sensitivity of our cost function to the weighted input for neuron $j$ at layer $l$. \n",
    "\n",
    "We then yield the following expressions to be used in our backwards propogation step in the block above:\n",
    "$\\begin{align*}\n",
    "& (1.0)\\hspace{5mm} \\delta^{[L]} = \\sigma^{'}(z^{[L]}) \\circ (a^{L} - y)\\\\\n",
    "& (1.1)\\hspace{5mm} \\delta^{[l]} = \\sigma^{'}(z^{[l]}) \\circ (W^{[l+1]})^{T} \\delta^{[l+1]}\\\\\n",
    "& (1.2)\\hspace{5mm} \\frac{\\partial C}{\\partial b_{j}^{[l]}} = \\delta_{j}^{[l]}\\\\\n",
    "& (1.3)\\hspace{5mm} \\frac{\\partial C}{\\partial w_{jk}^{[l]}} = \\delta_{j}^{[l]}a_{k}^{[l-1]}\n",
    "\\end{align*}$\n",
    "\n",
    "Where $\\circ$ is the hadamard product between two vecotrs, or the componentwise product of two vectors. \n",
    "\n",
    "These expressions are a direct results of derivations using the chain rule and can easily be proven and are shown in section 5 of $\\textit{Deep Learning for Applied Mathematicians}$\n",
    "\n",
    "In our block above, delta4 is the $L^{th}$ error function as shown in (1.0). \n",
    "\n",
    "delta3 and delta2 as we move back up our networks layers are shown through equation (1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #gradient step\n",
    "    W2 -= eta*delta2*xvec.T             #update our Weights and biases using loss function, or delta\n",
    "    W3 -= eta*delta3*a2.T               #weights and biases calculated and then entered into cost function to verify decay\n",
    "    W4 -= eta*delta4*a3.T\n",
    "    b2 -= eta*delta2\n",
    "    b3 -= eta*delta3\n",
    "    b4 -= eta*delta4\n",
    "    \n",
    "    savecost[counter] = cost(W2,W3,W4,b2,b3,b4,x1,x2,y)  #store our cost to observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) $\\hspace{5mm}$ In our last block, we go back through our weights and biases and update them in the $-\\nabla Cost(p)$ direction with the formula $$(1.4) \\hspace{5mm} p \\rightarrow p - \\eta \\nabla Cost(p)$$ as mentioned in block 5. Each weight matrix $W_{n}$ for $n = 2,...,L$  and bias vector $b_{n}$ for $n = 2,...,L$ is updated according to formula (1.4) for each of our prescribed gradient iterations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v2.2.0-gpu",
   "language": "python",
   "name": "tensorflow_gpu_2.2.0-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
